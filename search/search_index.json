{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pontoon","text":""},{"location":"#what-is-pontoon","title":"What is Pontoon?","text":"<p>Pontoon is an open source, self-hosted data export platform that is built from the ground up to help teams send data to their customers, at scale.</p> <p>\ud83d\udca1 Want to spin up Pontoon in just a few minutes? Try our Quick Start Guide!</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\ude80 Easy Deployment: Get started in minutes with Docker or deploy to AWS Fargate</li> <li>\ud83c\udfaf Major Data Warehouses Integrations: Supports Snowflake, Google BigQuery, Amazon Redshift, and Postgres as sources and destinations</li> <li>\u2601\ufe0f Multi-cloud: Send data from any cloud to any cloud. Amazon Redshift \u27a1\ufe0f Google BigQuery? No problem!</li> <li>\ud83c\udfd7\ufe0f Built for Scale: Sync over 1 million records per minute</li> <li>\u26a1 Automated Syncs: Schedule data transfers with automatic backfills and incremental loads</li> <li>\u2728 Web Interface: User-friendly dashboard for managing syncs, built with React/Nextjs</li> <li>\ud83d\udd0c REST API: Programmatic access to all Pontoon features, built with FastAPI</li> </ul>"},{"location":"#supported-platforms","title":"Supported Platforms","text":""},{"location":"#sources","title":"Sources","text":"<ul> <li>Data Warehouses: Snowflake, Google BigQuery, Amazon Redshift</li> <li>SQL Databases: Postgres</li> </ul>"},{"location":"#destinations","title":"Destinations","text":"<ul> <li>Data Warehouses: Snowflake, Google BigQuery, Amazon Redshift</li> <li>SQL Databases: Postgres</li> <li>Object Storage: Amazon S3, Google Cloud Storage, Azure Blob Storage</li> <li>Coming Soon - Data Lake Table Formats: Iceberg, Delta Lake and Hudi </li> </ul>"},{"location":"#the-problem-with-apis-data","title":"The Problem with APIs &amp; Data","text":"<p>We built Pontoon because traditional APIs are becoming increasingly problematic at modern data scale:</p> <ul> <li>Performance Issues: APIs struggle with large datasets and complex queries</li> <li>Poor Customer Experience: Customers have to spend weeks building ETLs or pay for managed ETL tools ($$$)</li> <li>Rate Limits: Data workloads tend to be bursty, often triggering rate limits, resulting in a frustrating experience for everyone involved</li> <li>Backfills: Backfilling historical data through APIs is inherently slow, as most APIs are optimized for real-time, not bulk ingestion</li> </ul> <p>Pontoon solves these problems with:</p> <ul> <li>Direct Warehouse Integration: Send data directly to customer's data warehouse. No more ETLs needed!</li> <li>Scalable Architecture: Handle millions of rows efficiently. Say goodbye to rate limits!</li> <li>Scheduled Syncs: Automate data delivery with automatic backfills on the first sync</li> <li>Self-Hosted: Full control over your data and infrastructure</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your deployment method:</p> <ol> <li>Quick Start - Get running in minutes</li> <li>Docker Compose - Local or production deployment</li> <li>AWS Fargate - Scalable cloud deployment</li> </ol>"},{"location":"getting-started/architecture/","title":"Architecture","text":"<p>Pontoon consists of several components that work together:</p> <ul> <li>Frontend: Next.js web application providing the user interface</li> <li>API: FastAPI backend handling HTTP requests and business logic</li> <li>Worker: Celery worker processing data transfers asynchronously</li> <li>Beat: Celery RedBeat scheduler managing recurring data transfers</li> <li>Postgres: Primary database storing configuration and metadata</li> <li>Redis: Message broker and cache for Celery tasks</li> </ul>"},{"location":"getting-started/aws-fargate/","title":"AWS Fargate Deployment","text":"<p>Deploy Pontoon on AWS Fargate for scalable, managed container orchestration with auto-scaling capabilities.</p>"},{"location":"getting-started/aws-fargate/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/aws-fargate/#what-youll-need","title":"What you'll need...","text":"<ul> <li>An AWS Account</li> <li>AWS credentials or IAM Role with permission to create VPCs, Subnets, Security Groups, ECS clusters, ECS Task Definitions and run ECS Tasks</li> <li>The AWS default <code>ecsTaskExecution</code> IAM role created in your account</li> <li>An AWS CloudWatch Log Group (we use one named <code>/ecs/pontoon-fargate</code> by default) </li> <li>Optional, but highly recommended: your own Redis 7+ and PostgreSQL 16+ instances</li> </ul> <p>\ud83d\udca1 This guide is intended to provide a very simple example of deloyment on AWS ECS Fargate. We strongly recommend that you adapt this guide to your environment and follow security and availability best practices as needed.</p>"},{"location":"getting-started/aws-fargate/#redis-and-postgresql","title":"Redis and PostgreSQL","text":"<p>We highly recommend using your own external Redis and PostgreSQL instances for security, reliability, durability and performance. </p> <p>To configure your own external data stores:</p> <ul> <li>Update the <code>FARGATE_POSTGRES_*</code> and <code>FARGATE_CELERY_*</code> variables in <code>.env</code> and ensure those endpoints are accessible from Subnets used by your ECS Task</li> <li>Remove the <code>redis</code> and <code>postgres</code> services from <code>docker-compose.fargate.yml</code></li> </ul>"},{"location":"getting-started/aws-fargate/#step-1-create-an-ecs-cluster","title":"Step 1: Create an ECS cluster","text":"<ul> <li>Install the Amazon ECS CLI</li> <li>Follow the instructions to configure your AWS credentials</li> <li>If you're using an existing AWS credentials profile, include the <code>--aws-profile &lt;name&gt;</code> on <code>ecs-cli</code> commands that follow   </li> </ul> <p>You can use an existing cluster, or create a new one: <pre><code>$ ecs-cli up --cluster pontoon --launch-type FARGATE\nINFO[0000] Created cluster cluster=pontoon region=us-west-2\nINFO[0001] Waiting for your cluster resources to be created... \nINFO[0001] Cloudformation stack status stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0ee3ff5453578221b\nSubnet created: subnet-0441b3260e16368bd\nSubnet created: subnet-04b7f89ee386776b6\nCluster creation succeeded.\n</code></pre></p>"},{"location":"getting-started/aws-fargate/#step-2-update-ecs-paramsyml","title":"Step 2: Update ecs-params.yml","text":"<p>Add your new or existing Subnet IDs to the ECS configuration file: <pre><code>...\n\nrun_params:\n  network_configuration:\n    awsvpc_configuration:\n      subnets:\n        - subnet-0441b3260e16368bd\n        - subnet-04b7f89ee386776b6\n      assign_public_ip: ENABLED\n\n...\n</code></pre></p>"},{"location":"getting-started/aws-fargate/#step-3-start-pontoon","title":"Step 3: Start Pontoon","text":"<p>Start Pontoon using ECS CLI and the Docker Compose definitions: <pre><code>$ ecs-cli compose -f docker-compose.fargate.yml up --launch-type FARGATE\nINFO[0000] Using ECS task definition                     TaskDefinition=\"Pontoon:1\"\nINFO[0000] Auto-enabling ECS Managed Tags               \nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/api\nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/beat\nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/frontend\nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/postgres\nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/redis\nINFO[0001] Starting container...                         container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/worker\nINFO[0001] Describe ECS container status                 container=pontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/redis desiredStatus=RUNNING lastStatus=PROVISIONING taskDefinition=\"Pontoon:1\"\n</code></pre></p> <p>Check that your task containers are up and running:</p> <pre><code>$ ecs-cli compose -f docker-compose.fargate.yml ps\nName    State    Ports    TaskDefinition  Health\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/redis     RUNNING  15.223.209.32:6379-&gt;6379/tcp  Pontoon:1       HEALTHY\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/frontend  RUNNING  15.223.209.32:3000-&gt;3000/tcp  Pontoon:1       UNKNOWN\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/api       RUNNING  15.223.209.32:8000-&gt;8000/tcp  Pontoon:1      UNKNOWN\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/worker    RUNNING                                Pontoon:1       UNKNOWN\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/postgres  RUNNING  15.223.209.32:5432-&gt;5432/tcp  Pontoon:1       HEALTHY\npontoon-fargate/4bbead2ac77a47c0b573d2481045d15d/beat      RUNNING                                Pontoon:1       UNKNOWN\n</code></pre>"},{"location":"getting-started/aws-fargate/#step-4-run-database-migrations","title":"Step 4: Run Database Migrations","text":"<p>Apply the database migrations to your <code>postgres</code> instance -- connection details will depend on how you've chosen to run PostgreSQL.</p> <pre><code>$ psql -h 15.224.219.33 -U dev -d pontoon -f api/db/migrations/V0001__initial_pontoon_schema.sql  \n</code></pre>"},{"location":"getting-started/aws-fargate/#step-5-access-the-web-ui","title":"Step 5: Access the Web UI","text":"<p>\ud83d\udca1 You may need to modify the Security Group for your ECS Task to allow external access to port 3000 </p> <p>Navigate to the public IP or DNS name for your ECS task using port <code>:3000</code>, e.g.:</p> <p>http://15.224.219.33:3000/</p> <p>Done! \ud83d\ude80</p>"},{"location":"getting-started/docker-compose/","title":"Docker Compose Deployment","text":"<p>Deploy Pontoon using Docker Compose for production environments with full control over your infrastructure.</p>"},{"location":"getting-started/docker-compose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker &amp; Docker Compose V2 (Install)</li> </ul>"},{"location":"getting-started/docker-compose/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/pontoon-data/Pontoon.git\n</code></pre>"},{"location":"getting-started/docker-compose/#step-2-environment-configuration-optional","title":"Step 2: Environment Configuration (Optional)","text":"<p>Update <code>.env</code> with any changes.</p> <p>If you are using a Postgres and/or Redis database that is external to Pontoon, update the relevant values in <code>.env</code>.</p> <p>\ud83d\udca1 Note: For production workloads, we recommend using external Postgres and Redis databases.</p>"},{"location":"getting-started/docker-compose/#step-3-build-and-run-pontoon","title":"Step 3: Build and Run Pontoon","text":"<pre><code>docker compose up --build\n</code></pre>"},{"location":"getting-started/docker-compose/#step-4-youre-ready","title":"Step 4: You're Ready! \ud83d\ude80","text":"<p>Navigate to <code>http://localhost:3000</code> to start exploring Pontoon or checkout the API docs at <code>http://localhost:8000/docs</code>.</p> <p>To learn about adding a source, check out the Sources &amp; Destinations documentation.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>To get Pontoon up and running in minutes, we packaged Pontoon into a single Docker container.</p> <p>\ud83d\udca1 Note: For production workloads, we recommend using external Postgres and Redis databases.</p>"},{"location":"getting-started/quick-start/#step-1-requirements","title":"Step 1: Requirements","text":"<ul> <li>Install Docker</li> </ul>"},{"location":"getting-started/quick-start/#step-2-run-pontoon","title":"Step 2: Run Pontoon","text":"<pre><code>docker run \\\n    --name pontoon \\\n    -p 3000:3000 \\\n    -p 8000:8000 \\\n    --rm \\\n    --pull=always \\\n    -v pontoon-internal-postgres:/var/lib/postgresql/data \\\n    -v pontoon-internal-redis:/data \\\n    ghcr.io/pontoon-data/pontoon/pontoon-unified:latest\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-youre-ready","title":"Step 3: You're Ready! \ud83d\ude80","text":"<p>Navigate to <code>http://localhost:3000</code> to start exploring Pontoon or checkout the API docs at <code>http://localhost:8000/docs</code>.</p> <p>The Transfer Quick Start Guide has the next steps for building your first data sync!</p> <p>To learn more about a specific source or destination, check out the Sources &amp; Destinations docs.</p>"},{"location":"getting-started/telemetry/","title":"Telemetry","text":"<p>By default, Pontoon collects anonymized usage data through PostHog to help us improve the performance and reliability of our tool. The data we collect includes general usage statistics and metadata such as transfer performance (e.g. error rates) to monitor the application\u2019s health and functionality.</p> <p>If you\u2019d like to disable all telemetry, you can do so by setting the environment variable <code>PONTOON_TELEMETRY_DISABLED</code> to <code>true</code>:</p> <pre><code>docker run \\\n  -e PONTOON_TELEMETRY_DISABLED=true \\\n  /* additional args */ \\\n  ghcr.io/pontoon-data/pontoon/pontoon-unified:latest\n</code></pre> <p>If you're running Pontoon with Docker Compose, set <code>PONTOON_TELEMETRY_DISABLED=true</code> in your <code>.env</code> file.</p> <p>If you disabled telemetry correctly, you'll see the following log when starting Pontoon: <code>Telemetry is disabled</code></p>"},{"location":"getting-started/transfer-quick-start/","title":"Transfer Quick Start","text":"<p>Here's how to quickly set up your first transfer in Pontoon. Follow these steps to get your data flowing from source to destination.</p>"},{"location":"getting-started/transfer-quick-start/#step-1-adding-a-source","title":"Step 1: Adding a Source","text":"<p>A source defines a connection to a database that holds data you want to sync. For detailed configuration instructions for each source type, see our Sources &amp; Destinations documentation.</p> <p>To add a source:</p> <ol> <li>Navigate to the Sources page in the Pontoon web interface</li> <li>Click \"Add New Source\"</li> <li>Select your source type (Snowflake, Redshift, BigQuery, etc.)</li> <li>Configure the connection details for your data source</li> <li>Test the connection to ensure it's working properly</li> </ol> <p></p>"},{"location":"getting-started/transfer-quick-start/#step-2-adding-a-model","title":"Step 2: Adding a Model","text":"<p>A model defines a dataset that is ready for export to recipients. Models are multi-tenant, with the <code>tenant_id</code> defining which row belongs to which customer.</p>"},{"location":"getting-started/transfer-quick-start/#important-model-fields","title":"Important Model Fields","text":"<p>When creating a model, you'll need to configure three critical fields:</p> <ul> <li> <p>Primary Key: A unique identifier for every row in your dataset. This field must be unique across all rows in a table.</p> </li> <li> <p>Last Modified Key: A timestamp field that indicates when each row was last updated. Pontoon uses this field to determine which rows need to be updated during future syncs.</p> </li> <li> <p>Tenant ID: An identifier used to associate data with specific customers. This same tenant ID will be used when adding recipients, creating the link between your data and who receives it.</p> </li> </ul> <p></p>"},{"location":"getting-started/transfer-quick-start/#step-3-adding-a-recipient","title":"Step 3: Adding a Recipient","text":"<p>Recipients are the customers or organizations that will receive your data. Each recipient is associated with a specific tenant ID.</p> <p>Important: The tenant ID you specify for a recipient must match the tenant ID used in any models that you want to send to this recipient. This creates the connection between your data models and the intended recipients.</p> <p>To add a recipient:</p> <ol> <li>Navigate to the Recipients page</li> <li>Click \"Add New Recipient\"</li> <li>Enter the recipient's details including their unique tenant ID</li> <li>Save the recipient configuration</li> </ol> <p></p>"},{"location":"getting-started/transfer-quick-start/#step-4-adding-a-destination","title":"Step 4: Adding a Destination","text":"<p>A destination defines where your data will be sent. This could be a data warehouse or another database.</p> <p>For detailed configuration instructions for each destination type, see our Sources &amp; Destinations documentation.</p> <p>To add a destination:</p> <ol> <li>Navigate to the Destinations page</li> <li>Click \"Add New Destination\"</li> <li>Select your destination type (Snowflake, Redshift, BigQuery, etc.)</li> <li>Add a recipient</li> <li>Configure the connection details</li> <li>Test the connection to ensure it's working properly</li> </ol> <p></p>"},{"location":"getting-started/transfer-quick-start/#step-5-begin-a-transfer","title":"Step 5: Begin a Transfer","text":"<p>Once you have configured add a destination, it will kick off an initial sync to backfill data. Click on a destination and navigate to the transfers tab to see the status of the transfer</p>"},{"location":"getting-started/transfer-quick-start/#next-steps","title":"Next Steps","text":"<p>Congrats, you've added your first destination! Some things to try include</p> <ul> <li>Add more models to share additional datasets</li> <li>Add more recipients / destinations to share data with additional customers</li> </ul>"},{"location":"sources-destinations/overview/","title":"Sources &amp; Destinations Overview","text":"<p>Pontoon supports a wide range of data sources and destinations, allowing you to transfer data between different platforms seamlessly.</p>"},{"location":"sources-destinations/overview/#supported-platforms","title":"Supported Platforms","text":""},{"location":"sources-destinations/overview/#sources","title":"Sources","text":"<p>Pontoon can read data from the following sources:</p> <ul> <li>Data Warehouses: Snowflake, Amazon Redshift, Google BigQuery</li> <li>SQL Databases: Postgres</li> </ul>"},{"location":"sources-destinations/overview/#destinations","title":"Destinations","text":"<p>Pontoon can write data to the following destinations:</p> <ul> <li>Data Warehouses: Snowflake, Amazon Redshift, Google BigQuery</li> <li>SQL Databases: Postgres</li> <li>Object Storage: Amazon S3, Google Cloud Storage, Azure Blob Storage</li> <li>Coming Soon - Data Lake Table Formats: Iceberg, Delta Lake and Hudi </li> </ul>"},{"location":"sources-destinations/destinations/S3/","title":"S3 Destination","text":"<p>Configure Amazon S3 as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/S3/#prerequisites","title":"Prerequisites","text":"<p>Before configuring S3 as a destination, ensure you have:</p> <ul> <li>AWS Account</li> <li>S3 Bucket: S3 bucket that data will be sent to</li> <li>Credentials: AWS Credentials with <code>s3:PutObject</code>, <code>s3:DeleteObject</code> permissions for your bucket</li> </ul>"},{"location":"sources-destinations/destinations/S3/#how-it-works","title":"How it works","text":"<p>The S3 connector writes data to your bucket as compressed Apache Parquet files using Apache Hive style partitions, which is supported by most query engines and data platforms making it easy to work with.  </p> <p>This connector is append-only, so re-running syncs will produce new files with a later timestamp and different batch ID (see below) but will not delete existing data in the destination.  </p>"},{"location":"sources-destinations/destinations/S3/#structure-of-landed-data","title":"Structure of landed data","text":"<p>Data written to S3 will have the following structure:</p> <p><code>s3://&lt;bucket_name&gt;/&lt;prefix&gt;/&lt;model&gt;/dt=&lt;transfer_date&gt;/&lt;transfer_timestamp&gt;_&lt;batch_id&gt;_&lt;file_index&gt;.parquet</code></p> <ul> <li><code>&lt;bucket_name&gt;</code> is the name of your S3 bucket</li> <li><code>&lt;prefix&gt;</code> is an optional folder prefix</li> <li><code>&lt;model&gt;</code> is the name of the data model transferred, similar to a table name</li> <li><code>&lt;transfer_date&gt;</code> is the date that the transfer started in the format <code>2025-01-01</code></li> <li><code>&lt;transfer_timestamp&gt;</code> is a timestand of when the transfer started in the format <code>20250301121507</code></li> <li><code>&lt;batch_id&gt;</code> is a batch ID generated by Pontoon that is unique to the running transfer - subsequent transfers to the same destination will have different batch IDs</li> <li><code>&lt;file_index&gt;</code> is a monotonically increasing integer for a given batch ID  </li> </ul>"},{"location":"sources-destinations/destinations/S3/#configuration","title":"Configuration","text":"Parameter Description Required Example <code>s3_bucket</code> S3 Bucket Name Yes <code>my-bucket</code> <code>s3_prefix</code> Folder path prefix Optional /exports <code>s3_region</code> Region of bucket Yes us-east-1 <code>aws_access_key_id</code> AWS access key ID Yes <code>AKIAIOSFODNN7EXAMPLE</code> <code>aws_secret_access_key</code> AWS secret access key Yes <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code>"},{"location":"sources-destinations/destinations/S3/#setup-destination","title":"Setup Destination","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select S3 as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/azure-blob-storage/","title":"Azure Blob Storage Destination","text":"<p>Configure Azure Blob Storage as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/azure-blob-storage/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Azure Blob Storage as a destination, ensure you have:</p> <ul> <li>Azure Account</li> <li>Azure Storage Account: An Azure Storage account with Hierarchical Namespace enabled</li> <li>Storage Container: Go to Storage account &gt; Data storage &gt; Containers and create a new container if needed </li> <li>Shared Access Signature (SAS): Connection String URI for Azure Storage with permission to read and write Blobs and Files in your container (Storage account &gt; Security + networking &gt; Shared access signature)</li> </ul>"},{"location":"sources-destinations/destinations/azure-blob-storage/#how-it-works","title":"How it works","text":"<p>The Azure Blob Storage connector writes data to your bucket as compressed Apache Parquet files using Apache Hive style partitions, which is supported by most query engines and data platforms making it easy to work with.  </p> <p>This connector is append-only, so re-running syncs will produce new files with a later timestamp and different batch ID (see below) but will not delete existing data in the destination.  </p>"},{"location":"sources-destinations/destinations/azure-blob-storage/#structure-of-landed-data","title":"Structure of landed data","text":"<p>Data written to Azure will have the following structure:</p> <p><code>abfss://&lt;blob_container&gt;/&lt;blob_prefix&gt;/&lt;model&gt;/dt=&lt;transfer_date&gt;/&lt;transfer_timestamp&gt;_&lt;batch_id&gt;_&lt;file_index&gt;.parquet</code></p> <ul> <li><code>&lt;blob_container&gt;</code> is the name of your Azure Storage Container</li> <li><code>&lt;blob_prefix&gt;</code> is an optional folder prefix</li> <li><code>&lt;model&gt;</code> is the name of the data model transferred, similar to a table name</li> <li><code>&lt;transfer_date&gt;</code> is the date that the transfer started in the format <code>2025-01-01</code></li> <li><code>&lt;transfer_timestamp&gt;</code> is a timestand of when the transfer started in the format <code>20250301121507</code></li> <li><code>&lt;batch_id&gt;</code> is a batch ID generated by Pontoon that is unique to the running transfer - subsequent transfers to the same destination will have different batch IDs</li> <li><code>&lt;file_index&gt;</code> is a monotonically increasing integer for a given batch ID  </li> </ul>"},{"location":"sources-destinations/destinations/azure-blob-storage/#configuration","title":"Configuration","text":"Parameter Description Required Example <code>blob_container</code> Azure Storage Container Name Yes <code>my-container</code> <code>blob_prefix</code> Folder path prefix Optional /exports <code>blob_connection_string</code> Azure SAS Connectio URI Yes <code>BlobEndpoint=https://&lt;account&gt;.blob.core.windows.net/</code>"},{"location":"sources-destinations/destinations/azure-blob-storage/#setup-destination","title":"Setup Destination","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select S3 as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/bigquery/","title":"Google BigQuery Destination","text":"<p>Configure Google BigQuery as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/bigquery/#prerequisites","title":"Prerequisites","text":"<p>Before configuring BigQuery as a destination, ensure you have:</p> <ul> <li>Google Cloud Project: Active GCP project with BigQuery enabled</li> <li>Service Account: Service account with appropriate BigQuery permissions</li> <li>Dataset: Target dataset for storing data</li> <li>Credentials: Service account key file or workload identity</li> <li>Network Access: Network connectivity to BigQuery (VPC connector or public internet)</li> <li>GCS Bucket: Google Cloud Storage bucket for staging data during transfer</li> </ul>"},{"location":"sources-destinations/destinations/bigquery/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/destinations/bigquery/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>project_id</code> Google Cloud project ID Yes <code>my-project-123</code> <code>dataset</code> BigQuery dataset ID Yes <code>customer_data</code> <code>target_schema</code> Target schema name Yes <code>export</code> <code>gcs_bucket</code> GCS bucket for staging data Yes <code>gs://mybucket</code> <code>gcs_prefix</code> GCS bucket prefix for data files Yes <code>/exports</code> <code>service_account</code> Service account credentials Yes JSON file"},{"location":"sources-destinations/destinations/bigquery/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/destinations/bigquery/#step-1-create-bigquery-resources","title":"Step 1: Create BigQuery Resources","text":""},{"location":"sources-destinations/destinations/bigquery/#create-dataset","title":"Create Dataset","text":"<pre><code># Create dataset\nbq mk --location=US my-project-123:customer_data\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#create-service-account","title":"Create Service Account","text":"<pre><code># Create service account\ngcloud iam service-accounts create pontoon-sa \\\n  --display-name=\"Pontoon Service Account\" \\\n  --project=my-project-123\n\n# Get service account email\nSA_EMAIL=$(gcloud iam service-accounts list \\\n  --filter=\"displayName:Pontoon Service Account\" \\\n  --format=\"value(email)\")\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#assign-bigquery-permissions","title":"Assign BigQuery Permissions","text":"<pre><code># Grant BigQuery Data Editor role\ngcloud projects add-iam-policy-binding my-project-123 \\\n  --member=\"serviceAccount:$SA_EMAIL\" \\\n  --role=\"roles/bigquery.dataEditor\"\n\n# Grant BigQuery Job User role\ngcloud projects add-iam-policy-binding my-project-123 \\\n  --member=\"serviceAccount:$SA_EMAIL\" \\\n  --role=\"roles/bigquery.jobUser\"\n\n# Grant BigQuery User role\ngcloud projects add-iam-policy-binding my-project-123 \\\n  --member=\"serviceAccount:$SA_EMAIL\" \\\n  --role=\"roles/bigquery.user\"\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#create-service-account-key","title":"Create Service Account Key","text":"<pre><code># Create and download service account key\ngcloud iam service-accounts keys create pontoon-sa-key.json \\\n  --iam-account=$SA_EMAIL\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#step-2-configure-google-cloud-storage","title":"Step 2: Configure Google Cloud Storage","text":""},{"location":"sources-destinations/destinations/bigquery/#create-gcs-bucket","title":"Create GCS Bucket","text":"<pre><code># Create GCS bucket for staging data\ngsutil mb -l US gs://my-pontoon-bucket\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#grant-gcs-permissions","title":"Grant GCS Permissions","text":"<pre><code># Grant Storage Object Admin role to service account\ngcloud projects add-iam-policy-binding my-project-123 \\\n  --member=\"serviceAccount:$SA_EMAIL\" \\\n  --role=\"roles/storage.objectAdmin\"\n\n# Grant Storage Object Viewer role to service account\ngcloud projects add-iam-policy-binding my-project-123 \\\n  --member=\"serviceAccount:$SA_EMAIL\" \\\n  --role=\"roles/storage.objectViewer\"\n</code></pre>"},{"location":"sources-destinations/destinations/bigquery/#step-3-configure-pontoon","title":"Step 3: Configure Pontoon","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select BigQuery as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/google-cloud-storage/","title":"Google Cloud Storage Destination","text":"<p>Configure Google Cloud Storage as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/google-cloud-storage/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Google Cloud Storage as a destination, ensure you have:</p> <ul> <li>GCP Account</li> <li>GCS Bucket: GCS bucket that data will be sent to</li> <li>Service Account: GCP Service Account credentials permission to read and write to your bucket</li> </ul>"},{"location":"sources-destinations/destinations/google-cloud-storage/#how-it-works","title":"How it works","text":"<p>The GCS connector writes data to your bucket as compressed Apache Parquet files using Apache Hive style partitions, which is supported by most query engines and data platforms making it easy to work with.  </p> <p>This connector is append-only, so re-running syncs will produce new files with a later timestamp and different batch ID (see below) but will not delete existing data in the destination.  </p>"},{"location":"sources-destinations/destinations/google-cloud-storage/#structure-of-landed-data","title":"Structure of landed data","text":"<p>Data written to GCS will have the following structure:</p> <p><code>gs://&lt;bucket_name&gt;/&lt;prefix&gt;/&lt;model&gt;/dt=&lt;transfer_date&gt;/&lt;transfer_timestamp&gt;_&lt;batch_id&gt;_&lt;file_index&gt;.parquet</code></p> <ul> <li><code>&lt;bucket_name&gt;</code> is the name of your GCS bucket</li> <li><code>&lt;prefix&gt;</code> is an optional folder prefix</li> <li><code>&lt;model&gt;</code> is the name of the data model transferred, similar to a table name</li> <li><code>&lt;transfer_date&gt;</code> is the date that the transfer started in the format <code>2025-01-01</code></li> <li><code>&lt;transfer_timestamp&gt;</code> is a timestand of when the transfer started in the format <code>20250301121507</code></li> <li><code>&lt;batch_id&gt;</code> is a batch ID generated by Pontoon that is unique to the running transfer - subsequent transfers to the same destination will have different batch IDs</li> <li><code>&lt;file_index&gt;</code> is a monotonically increasing integer for a given batch ID  </li> </ul>"},{"location":"sources-destinations/destinations/google-cloud-storage/#configuration","title":"Configuration","text":"Parameter Description Required Example <code>gcs_bucket_name</code> GCS Bucket Name Yes <code>my-bucket</code> <code>gcs_bucket_path</code> Folder path prefix Optional /exports <code>service_account</code> Service account credentials Yes JSON file"},{"location":"sources-destinations/destinations/google-cloud-storage/#setup-destination","title":"Setup Destination","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select S3 as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/postgresql/","title":"PostgreSQL Destination","text":"<p>Configure PostgreSQL as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/postgresql/#prerequisites","title":"Prerequisites","text":"<p>Before configuring PostgreSQL as a destination, ensure you have:</p> <ul> <li>PostgreSQL Server: Running PostgreSQL instance (local or cloud)</li> <li>Database: Target database for storing data</li> <li>Schema: Target schema within the database</li> <li>User Credentials: Database username and password</li> <li>Network Access: Network connectivity to PostgreSQL</li> </ul>"},{"location":"sources-destinations/destinations/postgresql/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/destinations/postgresql/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>host</code> PostgreSQL host address Yes <code>my-postgres-host.com</code> <code>port</code> PostgreSQL port Yes <code>5432</code> <code>database</code> Target database name Yes <code>analytics</code> <code>schema</code> Target schema name Yes <code>raw_data</code> <code>user</code> Database username Yes <code>pontoon_user</code> <code>password</code> Database password Yes <code>your_password</code>"},{"location":"sources-destinations/destinations/postgresql/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/destinations/postgresql/#step-1-create-postgresql-resources","title":"Step 1: Create PostgreSQL Resources","text":""},{"location":"sources-destinations/destinations/postgresql/#create-user","title":"Create User","text":"<pre><code>-- Create user\nCREATE USER pontoon_user WITH PASSWORD 'your_secure_password';\n\n-- Create schema\nCREATE SCHEMA raw_data;\n\n-- Grant permissions\nGRANT CONNECT ON DATABASE ... TO pontoon_user;\nGRANT USAGE ON SCHEMA raw_data TO pontoon_user;\nGRANT CREATE ON SCHEMA raw_data TO pontoon_user;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA raw_data TO pontoon_user;\n</code></pre>"},{"location":"sources-destinations/destinations/postgresql/#step-2-configure-pontoon","title":"Step 2: Configure Pontoon","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select PostgreSQL as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/redshift/","title":"Amazon Redshift Destination","text":"<p>Configure Amazon Redshift as a destination for your data transfers in Pontoon. Data is loaded via S3 for improved performance.</p>"},{"location":"sources-destinations/destinations/redshift/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Redshift as a destination, ensure you have:</p> <ul> <li>Recipient: Recipient defined with the correct Tenant ID</li> <li>AWS Account: Active AWS account with Redshift access</li> <li>Redshift Cluster: Running Redshift cluster</li> <li>Database: Target database within the cluster</li> <li>Schema: Target schema within the database</li> <li>User Credentials: Database username and password</li> <li>Network Access: VPC connectivity or public access</li> <li>S3 Bucket: S3 bucket for staging data during transfer</li> <li>IAM Role: IAM role associated with Redshift cluster with permissions to load data from the S3 bucket</li> </ul>"},{"location":"sources-destinations/destinations/redshift/#how-it-works","title":"How it works","text":"<p>The Redshift destination connector will perform the required DDL operations to replicate tables, transfer data by writing to an S3 location associated with the destination Redshift cluster, and run a <code>COPY</code> command to load the data.</p>"},{"location":"sources-destinations/destinations/redshift/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/destinations/redshift/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>hostname</code> Redshift cluster endpoint Yes <code>my-cluster.abc123.us-east-1.redshift.amazonaws.com</code> <code>port</code> Redshift port Yes <code>5439</code> <code>database</code> Target database name Yes <code>analytics</code> <code>user</code> Database username Yes <code>pontoon_user</code> <code>password</code> Database password Yes <code>your_secure_password</code> <code>target_schema</code> Target schema name Yes <code>export</code> <code>s3_region</code> S3 region Yes <code>us-east-1</code> <code>s3_bucket</code> S3 bucket for staging data Yes <code>s3://mybucket</code> <code>s3_prefix</code> S3 bucket prefix for data files Yes <code>/exports</code> <code>iam_role</code> IAM role ARN for Redshift-S3 access Yes <code>arn:aws:iam::123456789012:role/RedshiftS3Role</code> <code>aws_access_key_id</code> AWS access key ID Yes <code>AKIAIOSFODNN7EXAMPLE</code> <code>aws_secret_access_key</code> AWS secret access key Yes <code>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</code>"},{"location":"sources-destinations/destinations/redshift/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/destinations/redshift/#step-1-create-database-user","title":"Step 1: Create Database User","text":"<pre><code>-- Create user for Pontoon\nCREATE USER pontoon_user PASSWORD 'your_secure_password';\n\n-- Create schema\nCREATE SCHEMA raw_data;\n\n-- Grant permissions\nGRANT USAGE ON SCHEMA raw_data TO pontoon_user;\nGRANT CREATE ON SCHEMA raw_data TO pontoon_user;\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA raw_data TO pontoon_user;\n</code></pre>"},{"location":"sources-destinations/destinations/redshift/#step-2-configure-aws-resources","title":"Step 2: Configure AWS Resources","text":"<p>Create S3 Bucket: Create an S3 bucket for staging data during transfers</p> <pre><code>aws s3 mb s3://your-pontoon-bucket --region us-east-1\n</code></pre> <p>Create IAM Role: Create an IAM role with the following permissions:</p> <p><pre><code>{\n   \"Effect\": \"Allow\",\n   \"Action\": [\n     \"s3:GetObject\"\n   ],\n   \"Resource\": \"arn:aws:s3:::your-pontoon-bucket/*\"\n }\n}\n</code></pre>    - Attach this role to your Redshift cluster</p> <p>Create AWS Access Keys: Create AWS access keys with permission to read and write to your S3 bucket. </p>"},{"location":"sources-destinations/destinations/redshift/#step-3-configure-pontoon","title":"Step 3: Configure Pontoon","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select Redshift as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/destinations/snowflake/","title":"Snowflake Destination","text":"<p>Configure Snowflake as a destination for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/destinations/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Snowflake as a destination, ensure you have:</p> <ul> <li>Recipient: Recipient defined with the correct Tenant ID</li> <li>Snowflake Account: Active Snowflake destination account with appropriate permissions</li> <li>Warehouse: Snowflake warehouse for compute resources</li> <li>Database: Target database within the cluster</li> <li>Schema: Target schema within the database</li> <li>User Credentials: Username and password</li> <li>Network Access: Network connectivity to Snowflake (IP whitelist or private link)</li> </ul>"},{"location":"sources-destinations/destinations/snowflake/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/destinations/snowflake/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>account</code> Snowflake account identifier Yes <code>xy12345.us-east-1</code> <code>warehouse</code> Snowflake warehouse name Yes <code>PONTOON_WH</code> <code>database</code> Snowflake database name Yes <code>PONTOON</code> <code>schema</code> Target schema name Yes <code>EXPORT</code> <code>user</code> Snowflake username Yes <code>PONTOON_USER</code> <code>password</code> Snowflake password Yes <code>your_password</code>"},{"location":"sources-destinations/destinations/snowflake/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/destinations/snowflake/#step-1-create-snowflake-resources","title":"Step 1: Create Snowflake Resources","text":""},{"location":"sources-destinations/destinations/snowflake/#create-warehouse","title":"Create Warehouse","text":"<pre><code>-- Create a warehouse for Pontoon\nCREATE WAREHOUSE PONTOON_WH\n  WAREHOUSE_SIZE = 'X-SMALL'\n  AUTO_SUSPEND = 60\n  AUTO_RESUME = TRUE\n  MIN_CLUSTER_COUNT = 1\n  MAX_CLUSTER_COUNT = 1;\n</code></pre>"},{"location":"sources-destinations/destinations/snowflake/#create-database-and-schema","title":"Create Database and Schema","text":"<pre><code>-- Create database for customer data\nCREATE DATABASE CUSTOMER_DATA;\n\n-- Use the database\nUSE DATABASE CUSTOMER_DATA;\n\n-- Create schema for raw data\nCREATE SCHEMA EXPORT;\n</code></pre>"},{"location":"sources-destinations/destinations/snowflake/#create-user-and-role","title":"Create User and Role","text":"<pre><code>-- Create role for Pontoon\nCREATE ROLE PONTOON_ROLE;\n\n-- Create user for Pontoon\nCREATE USER PONTOON_USER\n  PASSWORD = 'your_secure_password'\n  DEFAULT_ROLE = PONTOON_ROLE\n  DEFAULT_WAREHOUSE = PONTOON_WH;\n\n-- Grant warehouse usage\nGRANT USAGE ON WAREHOUSE PONTOON_WH TO ROLE PONTOON_ROLE;\n\n-- Grant database and schema permissions\nGRANT USAGE ON DATABASE CUSTOMER_DATA TO ROLE PONTOON_ROLE;\nGRANT USAGE ON SCHEMA CUSTOMER_DATA.EXPORT TO ROLE PONTOON_ROLE;\n\n-- Grant table creation permissions\nGRANT CREATE TABLE ON SCHEMA CUSTOMER_DATA.EXPORT TO ROLE PONTOON_ROLE;\n\n-- Grant data loading permissions\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA CUSTOMER_DATA.EXPORT TO ROLE PONTOON_ROLE;\nGRANT INSERT, UPDATE, DELETE ON FUTURE TABLES IN SCHEMA CUSTOMER_DATA.EXPORT TO ROLE PONTOON_ROLE;\n\n-- Assign role to user\nGRANT ROLE PONTOON_ROLE TO USER PONTOON_USER;\n</code></pre>"},{"location":"sources-destinations/destinations/snowflake/#step-2-configure-pontoon","title":"Step 2: Configure Pontoon","text":"<ol> <li>Navigate to Destinations \u2192 New Destination</li> <li>Select Snowflake as the destination type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the destination</li> </ol>"},{"location":"sources-destinations/sources/bigquery/","title":"Google BigQuery Source","text":"<p>Configure Google BigQuery as a source for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/sources/bigquery/#prerequisites","title":"Prerequisites","text":"<p>Before configuring BigQuery as a source, ensure you have:</p> <ul> <li>Google Cloud Project: Active GCP project with BigQuery enabled</li> <li>Service Account: Service account with appropriate BigQuery permissions</li> <li>Dataset: Dataset for storing data</li> <li>Credentials: Service account JSON key file </li> <li>Network Access: Network connectivity to BigQuery (VPC connector or public internet)</li> </ul>"},{"location":"sources-destinations/sources/bigquery/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/sources/bigquery/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>project_id</code> Google Cloud project ID Yes <code>my-project-123</code> <code>dataset</code> BigQuery dataset ID Yes <code>customer_data</code> <code>credentials</code> Service account credentials Yes JSON file"},{"location":"sources-destinations/sources/bigquery/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/sources/bigquery/#step-1-create-bigquery-resources","title":"Step 1: Create BigQuery Resources","text":"<pre><code>gcloud init # authenticate with a BigQuery admin role\nbq mk --dataset --description \"Pontoon Data\" --location=US my-project-123:pontoon_data\n</code></pre>"},{"location":"sources-destinations/sources/bigquery/#step-2-create-a-service-account","title":"Step 2: Create a service account","text":"<p>Create a service account with the <code>BigQuery Data Viewer</code> and <code>BigQuery Metadata Viewer</code> IAM roles attached and download the <code>service-account.json</code> key file.</p>"},{"location":"sources-destinations/sources/bigquery/#step-3-configure-pontoon","title":"Step 3: Configure Pontoon","text":"<ol> <li>Navigate to Sources \u2192 New Source</li> <li>Select BigQuery as the source type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the source</li> </ol>"},{"location":"sources-destinations/sources/postgresql/","title":"Postgres Source","text":"<p>Configure Postgres as a source for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/sources/postgresql/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Postgres as a destination, ensure you have:</p> <ul> <li>Postgres Server: Running Postgres instance (local or cloud)</li> <li>Database: Target database for storing data</li> <li>User Credentials: Database username and password</li> <li>Network Access: Network connectivity to Postgres</li> </ul>"},{"location":"sources-destinations/sources/postgresql/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/sources/postgresql/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>hostname</code> Postgres host address Yes <code>my-postgres-host.com</code> <code>port</code> Postgres port Yes <code>5432</code> <code>database</code> Target database name Yes <code>analytics</code> <code>user</code> Database username Yes <code>pontoon_user</code> <code>password</code> Database password Yes <code>your_password</code>"},{"location":"sources-destinations/sources/postgresql/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/sources/postgresql/#step-1-create-postgresql-resources","title":"Step 1: Create PostgreSQL Resources","text":""},{"location":"sources-destinations/sources/postgresql/#create-database-and-user","title":"Create Database and User","text":"<pre><code>-- Create database\nCREATE DATABASE analytics;\n\n-- Create user\nCREATE USER pontoon_user WITH PASSWORD 'your_secure_password';\n\n-- Create schema\nCREATE SCHEMA pontoon_data;\n\n-- Grant permissions\nGRANT CONNECT ON DATABASE analytics TO pontoon_user;\nGRANT USAGE ON SCHEMA pontoon_data TO pontoon_user;\nGRANT SELECT ON ALL TABLES IN SCHEMA pontoon_data TO pontoon_user;\n\n-- Optionally, grant future permissions\nALTER DEFAULT PRIVILEGES IN SCHEMA pontoon_data GRANT SELECT ON TABLES TO pontoon_user;\n</code></pre>"},{"location":"sources-destinations/sources/postgresql/#step-2-configure-pontoon","title":"Step 2: Configure Pontoon","text":"<ol> <li>Navigate to Sources \u2192 New Source</li> <li>Select Postgres as the source type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the source</li> </ol>"},{"location":"sources-destinations/sources/redshift/","title":"Amazon Redshift Source","text":"<p>Configure Amazon Redshift as a source for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/sources/redshift/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Redshift as a destination, ensure you have:</p> <ul> <li>AWS Account: Active AWS account with Redshift access</li> <li>Redshift Cluster: Running Redshift cluster</li> <li>Database: Target database within the cluster</li> <li>User Credentials: Database username and password</li> <li>Network Access: VPC connectivity or public access</li> </ul>"},{"location":"sources-destinations/sources/redshift/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/sources/redshift/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>hostname</code> Redshift cluster endpoint Yes <code>my-cluster.abc123.us-east-1.redshift.amazonaws.com</code> <code>port</code> Redshift port Yes <code>5439</code> <code>database</code> Target database name Yes <code>analytics</code> <code>user</code> Database username Yes <code>pontoon_user</code> <code>password</code> Database password Yes <code>your_secure_password</code>"},{"location":"sources-destinations/sources/redshift/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/sources/redshift/#step-1-create-database-user","title":"Step 1: Create Database User","text":"<pre><code>-- Create user for Pontoon\nCREATE USER pontoon_user PASSWORD 'your_secure_password';\n\n-- Create schema\nCREATE SCHEMA pontoon_data;\n\n-- Grant permissions\nGRANT USAGE ON SCHEMA pontoon_data TO pontoon_user;\nGRANT SELECT ON SCHEMA pontoon_data TO pontoon_user;\n</code></pre>"},{"location":"sources-destinations/sources/redshift/#step-2-configure-pontoon","title":"Step 2: Configure Pontoon","text":"<ol> <li>Navigate to Sources \u2192 New Source</li> <li>Select Redshift as the source type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the source</li> </ol>"},{"location":"sources-destinations/sources/snowflake/","title":"Snowflake Source","text":"<p>Configure Snowflake as a source for your data transfers in Pontoon.</p>"},{"location":"sources-destinations/sources/snowflake/#prerequisites","title":"Prerequisites","text":"<p>Before configuring Snowflake as a destination, ensure you have:</p> <ul> <li>Snowflake Account: Active Snowflake account with appropriate permissions</li> <li>Warehouse: Snowflake warehouse for compute resources</li> <li>Database: Database to store Pontoon data</li> <li>Schema: Target schema within the database</li> <li>User Credentials: Username and password</li> <li>Network Access: Network connectivity to Snowflake</li> </ul>"},{"location":"sources-destinations/sources/snowflake/#configuration","title":"Configuration","text":""},{"location":"sources-destinations/sources/snowflake/#connection-details","title":"Connection Details","text":"Parameter Description Required Example <code>account</code> Snowflake account identifier Yes <code>xy12345.us-east-1</code> <code>database</code> Snowflake database name Yes <code>pontoon</code> <code>warehouse</code> Snowflake warehouse name Yes <code>PONTOON_WH</code> <code>user</code> Snowflake username Yes <code>PONTOON_USER</code> <code>access_token</code> Snowflake access token Yes <code>af877f76...</code>"},{"location":"sources-destinations/sources/snowflake/#setup-instructions","title":"Setup Instructions","text":""},{"location":"sources-destinations/sources/snowflake/#step-1-create-snowflake-resources","title":"Step 1: Create Snowflake Resources","text":""},{"location":"sources-destinations/sources/snowflake/#create-database-and-schema","title":"Create Database and Schema","text":"<pre><code>-- Optionally, create database for customer data\nCREATE DATABASE PONTOON;\n\n-- Use the database\nUSE DATABASE PONTOON;\n\n-- Create schema for raw data\nCREATE SCHEMA PONTOON_DATA;\n</code></pre>"},{"location":"sources-destinations/sources/snowflake/#create-user-and-role","title":"Create User and Role","text":"<pre><code>-- Create role for Pontoon\nCREATE ROLE PONTOON_ROLE;\n\n-- Create user for Pontoon\nCREATE USER PONTOON_USER\n  PASSWORD = 'your_secure_password'\n  DEFAULT_ROLE = PONTOON_ROLE\n  DEFAULT_WAREHOUSE = &lt;YOUR_WAREHOUSE&gt;;\n\n-- Grant database and schema permissions\nGRANT USAGE ON DATABASE PONTOON TO ROLE PONTOON_ROLE;\nGRANT USAGE ON SCHEMA PONTOON.PONTOON_DATA TO ROLE PONTOON_ROLE;\nGRANT SELECT ON ALL TABLES IN SCHEMA PONTOON.PONTOON_DATA TO ROLE PONTOON_ROLE;\n\n-- Assign role to user\nGRANT ROLE PONTOON_ROLE TO USER PONTOON_USER;\n</code></pre>"},{"location":"sources-destinations/sources/snowflake/#step-2-configure-access-token","title":"Step 2: Configure access token","text":"<p>Create a Snowflake Access Token (Admin &gt; Users &amp; Roles &gt; Programmatic access tokens) for <code>PONTOON_USER</code> and ensure you have a Snowflake Network Policy in place that allows <code>PONTOON_USER</code> to access your Snowflake instance using the access token.</p>"},{"location":"sources-destinations/sources/snowflake/#step-2-configure-pontoon","title":"Step 2: Configure Pontoon","text":"<ol> <li>Navigate to Sources \u2192 New Source</li> <li>Select Snowflake as the source type</li> <li>Enter connection details:</li> <li>Click Test Connection to verify</li> <li>Click Save to create the source</li> </ol>"}]}